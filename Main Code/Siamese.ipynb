{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devloper13/SiameseNetworkProject/blob/master/Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "o5SKv74zumzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Siamese Network Code \n",
        "\n",
        "The following modules in python has been used \n",
        "<ul>\n",
        "  <li>Tensorflow for constructing Siamese Networks</li>\n",
        "  <li>NLTK library for stemming and stop words removal</li>\n",
        "  <li>Google Drive library for data strage</li>\n",
        "  <li>Scikit, Numpy and Pandas for misc. tensor computations</li>\n",
        "<ul>\n",
        "  "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sWwewmyoKBOK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from builtins import input\n",
        "\n",
        "#import system things\n",
        "from tensorflow.examples.tutorials.mnist import input_data # for data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cd76XW7KpHCy",
        "outputId": "5db2d1f6-78e9-4cc3-f67e-0b340cc2459f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoYoHnhXx2zt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idim = 30628\n",
        "\n",
        "class Siamese:\n",
        "\n",
        "    # Create model\n",
        "    def __init__(self,dname=\"Siamese\"):\n",
        "        self.x1 = tf.placeholder(tf.float32, [None, idim])\n",
        "        self.x2 = tf.placeholder(tf.float32, [None, idim])\n",
        "\n",
        "        with tf.variable_scope(dname) as scope:\n",
        "            self.o1 = self.network(self.x1)\n",
        "            scope.reuse_variables()\n",
        "            self.o2 = self.network(self.x2)\n",
        "\n",
        "        # Create loss\n",
        "        self.y_ = tf.placeholder(tf.float32, [None])\n",
        "        #self.y_ = tf.placeholder(tf.int32, [None])\n",
        "        self.loss = self.cosineLoss()\n",
        "\n",
        "    #create the network \n",
        "    def network(self, x):\n",
        "        \n",
        "        x = tf.reshape(x,shape=[-1,1,idim,1])\n",
        "        activated_conv1 = self.conv_layer('conv_1',x,3)\n",
        "        maxpool1 = self.maxpool_layer('maxp_1',activated_conv1)\n",
        "        \n",
        "        flattened_conv = tf.layers.flatten(maxpool1)   #To be removed\n",
        "        activated_fc1 = self.fc_layer( \"fc1\",flattened_conv, 128)\n",
        "        #activated_fc2 = self.fc_layer(\"fc2\",activated_fc1, 1024)\n",
        "        #activated_fc3 = self.fc_layer(\"fc3\",activated_fc2, 2)\n",
        "        \n",
        "        return activated_fc1\n",
        "        \n",
        "    #create the convolution layer \n",
        "    def conv_layer(self,name,inputs,cur_channel):\n",
        "        #print(inputs.get_shape())\n",
        "        prev_channel = inputs.get_shape()[-1]\n",
        "        #print(prev_channel)\n",
        "        init = tf.variance_scaling_initializer(scale=2.0)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[1,10,prev_channel,cur_channel],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_channel],initializer = init)\n",
        "        conv = tf.nn.conv2d(inputs,w,strides=[1,1,1,1],padding = \"SAME\")\n",
        "        activation = conv+b\n",
        "        return activation  \n",
        "      \n",
        "    def maxpool_layer(self,name,inputs):\n",
        "        return tf.nn.relu(tf.nn.max_pool(inputs,ksize=[1,1,100,1],strides=[1,1,100,1],padding=\"SAME\"))  \n",
        "    \n",
        "    def fc_layer(self,name,inputs,cur_layer):\n",
        "        print(inputs.get_shape())\n",
        "        prev_layer = inputs.get_shape()[-1]\n",
        "        init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "        w = tf.get_variable(name+\"_w\",dtype=tf.float32,shape=[prev_layer,cur_layer],initializer=init)\n",
        "        b = tf.get_variable(name+\"_b\",dtype=tf.float32,shape=[cur_layer],initializer=init)\n",
        "        activation = tf.matmul(inputs,w)+b\n",
        "        return activation\n",
        "    \n",
        "        \n",
        "    def cosineLoss(self):\n",
        "        \n",
        "        norms1 = tf.norm(self.o1,axis=1)\n",
        "        norms2 = tf.norm(self.o2,axis=1)\n",
        "        norm = tf.multiply(norms1,norms2)\n",
        "        cosines = tf.div(tf.reduce_sum(tf.multiply(self.o1,self.o2),axis=1),norm)\n",
        "        \n",
        "        labels_t = self.y_\n",
        "        labels_f = tf.subtract(1.0, self.y_, name=\"1-yi\")          # labels_ = !labels;\n",
        "        \n",
        "        \n",
        "        C = tf.constant(0.5, name=\"C\")\n",
        "        \n",
        "        pos = tf.multiply(labels_t,tf.subtract(1.0,cosines), name=\"yi_x_cosine\")\n",
        "        \n",
        "        neg = tf.multiply(labels_f, tf.maximum(tf.subtract(cosines,C),0), name=\"Nyi_x_C-cosine\")\n",
        "        losses = tf.add(pos, neg, name=\"losses\")\n",
        "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
        "        return loss\n",
        "        \n",
        "\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GNX5a8KeKBOu",
        "outputId": "3539073c-dd0d-49bf-9ff6-cf1ffcbe677c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=1Jhj9OazxPnvLcuuZsZvNFfpnnsFg88I7\"\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('three_hash')  \n",
        "with open('three_hash','rb') as f:\n",
        "   three_hash_dict = pickle.load(f)\n",
        "print(len(three_hash_dict))"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kt-JQ95JojbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def getThreeHash(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer=vectorizer.build_tokenizer()\n",
        "  \n",
        "  hashes=\"\"\n",
        "  tokens=tokenizer(text)\n",
        "  ps = PorterStemmer()\n",
        "  tokens = [ps.stem(word) for word in tokens]\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      tokenModi=\"#\"+token+\"#\"\n",
        "      output = list(ngrams(tokenModi, 3))\n",
        "      for a in output:\n",
        "        hashes+=(''.join(a))+\" \"\n",
        "  \n",
        "  return(hashes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bFQrr4tINCx2",
        "outputId": "8bdc89a8-c13e-4bf2-9459-73e28a8235c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"https://drive.google.com/open?id=17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\" #create shareable link of google drive file\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('QA.csv')  \n",
        "ndata = pd.read_csv('QA.csv',error_bad_lines=False).values\n"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17VMN5CJA05vTPEs15gw-W2ocxmEITQEH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4OzxqKw47mcT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = ndata[:5000,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9u09aOmJXNfY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokenlower = [word.lower() for word in tokens]\n",
        "    \n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    tokenlower = [word for word in stopWords if word not in stopWords]\n",
        "    \n",
        "    tokenDict = nltk.defaultdict(int)\n",
        "    for word in tokens:\n",
        "        tokenDict[word] += 1\n",
        "    return tokenDict\n",
        "\n",
        "def cosDistance(v1,v2):\n",
        "    dotProduct = np.dot(v1,v2)\n",
        "    normV1 = np.linalg.norm(v1)\n",
        "    normV2 = np.linalg.norm(v2)\n",
        "#     print(dotProduct,normV1,normV2)\n",
        "    return dotProduct / (normV1 * normV2)\n",
        "\n",
        "def getSimilarity(vDict1,vDict2):\n",
        "    allWords = []\n",
        "    for key in vDict1:\n",
        "        allWords.append(key)\n",
        "    for key in vDict2:\n",
        "        allWords.append(key)\n",
        "    allWordsSize = len(allWords)\n",
        "    \n",
        "    v1 = np.zeros(allWordsSize,dtype=np.int)\n",
        "    v2 = np.zeros(allWordsSize,dtype=np.int)\n",
        "    \n",
        "    i = 0\n",
        "    for key in allWords:\n",
        "#         print(key)\n",
        "        v1[i] = vDict1.get(key, 0)\n",
        "        v2[i] = vDict2.get(key, 0)\n",
        "        i += 1\n",
        "#     print(v1,v2)    \n",
        "    return cosDistance(v1,v2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HwGPkK3SON0N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize(hashString,dictionary):\n",
        "  \n",
        "  vectorizer = CountVectorizer()\n",
        "  tokenizer = vectorizer.build_tokenizer()\n",
        "  vec = [0]*idim\n",
        "  \n",
        "  for token in tokenizer(hashString):\n",
        "    try:\n",
        "      vec[dictionary[token]] += 1\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "  return vec\n",
        "\n",
        "def createBatch(data,goodSet,badSetQues,badSetAns,dictionary):\n",
        "  \n",
        "  good_data = data[goodSet]\n",
        "  bad_quest = data[badSetQues,0]\n",
        "  bad_anser = data[badSetAns,1]\n",
        "  questions = np.concatenate((good_data[:,0],bad_quest))\n",
        "  answers = np.concatenate((good_data[:,1],bad_anser))\n",
        "  label = np.array([1]*good_data.shape[0] + [0]*bad_quest.shape[0])\n",
        "  \n",
        "  ques,ans = [],[]\n",
        "  for d in questions:\n",
        "    ques += [vectorize(getThreeHash(d.lower()),dictionary)]\n",
        "  for i,d in enumerate(answers):\n",
        "    ans += [vectorize(getThreeHash(str(d).lower()),dictionary)]\n",
        "  return np.array(ques),np.array(ans),label\n",
        "    \n",
        "def getRandomBatch(data,batchsize,dictionary,good_bad=0.5):\n",
        "  \n",
        "  goodSet = np.random.permutation(data.shape[0])[:int(batchsize*good_bad)]\n",
        "  badSetQues = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  badSetAns = np.random.permutation(data.shape[0])[:batchsize - int(batchsize*good_bad)]\n",
        "  return createBatch(data,goodSet,badSetQues,badSetAns,dictionary)\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fI2XdkYUpxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc18a0df-66bc-48dc-a65c-d7d97900080c"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "siamese = Siamese('siamese6')\n",
        "try:\n",
        "  sess.close()\n",
        "except:\n",
        "  pass\n",
        "sess = tf.InteractiveSession()\n",
        "train_Step = tf.train.MomentumOptimizer(0.01,0.05).minimize(siamese.loss)\n",
        "tf.initialize_all_variables().run()"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 921)\n",
            "(?, 921)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wXmBCVN0V3hv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSiameseNetwork(data,siamese,sess,batchSize,epochs,dictionary):\n",
        "  \n",
        "  #Siamese Network\n",
        "  #sess = tf.Session()\n",
        "  #tf.reset_default_graph()\n",
        "  #siamese = Siamese()\n",
        "  \n",
        "  while epochs > 0:\n",
        "    \n",
        "    avg = 0.0\n",
        "    permSet = np.random.permutation(data.shape[0])\n",
        "\n",
        "    for p in range(0,permSet.shape[0],batchSize):\n",
        "\n",
        "      goodSet = np.array(list(range(p,p+batchSize)))\n",
        "      badSetQ = goodSet.copy()\n",
        "      badSetA = np.array(permSet[p:p+batchSize])\n",
        "      ques,ans,labl = createBatch(data,goodSet,badSetQ,badSetA,dictionary)\n",
        "\n",
        "\n",
        "      #print(ques.shape,ans.shape,labl.shape)\n",
        "      _, Loss = sess.run([train_Step,siamese.loss], feed_dict={\n",
        "                    siamese.x1: ques,\n",
        "                    siamese.x2: ans,\n",
        "                    siamese.y_: labl})\n",
        "      avg += Loss\n",
        "      #print(\"Epoch\",epochs,\"Batch\",p/batchSize,\"Loss\",Loss)\n",
        "    \n",
        "    print(\"Average Epoch \",epochs,\"Loss \",avg/(data.shape[0]/batchSize))\n",
        "    epochs -= 1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8_vJYXycs0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(qset,ques,siamese,sess,dictionary,alpha=0.5,best=3):\n",
        "  \n",
        "  vques  = preprocess(ques)\n",
        "  thQues = np.array(vectorize(getThreeHash(ques.lower()),dictionary)).reshape(1,-1)\n",
        "\n",
        "  scores = []\n",
        "\n",
        "  for q in qset:\n",
        "\n",
        "    oQues = np.array(vectorize(getThreeHash(q.lower()),dictionary)).reshape(1,-1)\n",
        "    vQues = preprocess(q)\n",
        "    Loss = sess.run([siamese.loss],feed_dict={\n",
        "                    siamese.x1: thQues,\n",
        "                    siamese.x2: oQues,\n",
        "                    siamese.y_: np.array([1.0])})\n",
        "    tLoss = getSimilarity(vques,vQues)\n",
        "    #print(alpha*Loss[0],(1-alpha)*(1-tLoss))\n",
        "    scores += [alpha*Loss[0] + (1-alpha)*(1-tLoss)]\n",
        "\n",
        "  sscores = np.argsort(np.array(scores))[:best]\n",
        "  return qset[sscores],scores\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "scF-b7JY8NQW",
        "outputId": "79dc06e4-48d9-43a5-b919-2f22049074a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "trainSiameseNetwork(data,siamese,sess,100,30,three_hash_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Epoch  30 Loss  0.2242837056517601\n",
            "Average Epoch  29 Loss  0.21048615053296088\n",
            "Average Epoch  28 Loss  0.2039389818906784\n",
            "Average Epoch  27 Loss  0.20255933552980424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "InihtcorSzMn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainQues = data[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGJe9ZPRq7Rj",
        "colab_type": "code",
        "outputId": "92f3fb13-dca8-4c63-eb54-b9d8808a4a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - how to attract a girl\n",
            "Best Three Answers \n",
            "How should I act on a date?\n",
            "Is a transponder required to fly in class C airspace?\n",
            "How do I end a date with a girl I'm not interested in?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oWWsqQPa2Dzu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9843ee45-4bc4-4b9e-d6d4-bdd59b500564"
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - best place to eat\n",
            "Best Three Answers \n",
            "best place to meet guys in the bay area?\n",
            "Whats a good place to eat in LA?\n",
            "Where's the best place to get my FICO score?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lsISYpVV2RzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7466fbb4-0b30-440c-b6fe-38f78a51ea0c"
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - best internet site\n",
            "Best Three Answers \n",
            "what is the best news site on the net?\n",
            "How many websites are on the internet?\n",
            "What is the singel most important thing you are missing on the internet ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Efkkr3Gh6oOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "54d8578c-1b28-4405-e2cc-d7be818ee512"
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - find peace\n",
            "Best Three Answers \n",
            "Who won the first nobel peace prize?\n",
            "Where can I find help about a war?\n",
            "need to find reserch about work ethics?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qCp4Lupv7RUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "44a1bc1e-496f-463f-c2a0-76166374f8e9"
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - which fruit is good for health\n",
            "Best Three Answers \n",
            "Why is red wine good for your heart?\n",
            "Why are blueberries so good for your health?\n",
            "How do you get a toddler to eat what's good for him?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "40h8xem77nCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4e006089-24a2-45b5-c958-3445ae31dd09"
      },
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter a question - \")\n",
        "score = predict(trainQues,ques,siamese,sess,three_hash_dict,alpha=0.8)\n",
        "print(\"Best Three Answers \\n%s\\n%s\\n%s\" % (score[0][0],score[0][1],score[0][2]))"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question - Who is most famous celebrity\n",
            "Best Three Answers \n",
            "Who is the most famous woman athlete of all time?\n",
            "What is emo?\n",
            "What is \"I\"?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AW-oaJXB7z6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}